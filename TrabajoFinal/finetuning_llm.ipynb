{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Fine-tuning de LLM para Auditor√≠a de Calidad\n",
    "\n",
    "Este cuaderno permite entrenar un modelo de lenguaje (LLM) especializado en auditor√≠a de llamadas de servicio al cliente, utilizando los resultados generados por el pipeline anterior.\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- **Motor**: [Unsloth](https://github.com/unslothai/unsloth) (2x m√°s r√°pido, 70% menos memoria).\n",
    "- **M√©todo**: QLoRA (Fine-tuning eficiente en 4-bit).\n",
    "- **Modelos soportados**: Llama 3.1, Qwen 2.5, Mistral.\n",
    "- **Salida**: Formato GGUF para usar en **LM Studio**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. üìä Generaci√≥n de Data de Entrenamiento (Punto 0)\n",
    "En esta secci√≥n generamos el dataset para el fine-tuning combinando:\n",
    "1. **Reglas de Negocio**: Extra√≠das de `indicaciones_gestion_requerimiento.json`.\n",
    "2. **An√°lisis Reales**: Resultados previos de la carpeta `greeting_analysis`.\n",
    "3. **Escenarios Sint√©ticos**: Variaciones generadas para cubrir casos de cumplimiento e incumplimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generando 3000 ejemplos de entrenamiento con cobertura R1-R9...\n",
      "üéâ Dataset total construido con 3000 ejemplos (sint√©ticos + reales).\n",
      "üìÅ Archivo guardado en: data\\pomptsft\\dataset_audit.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "CRITERIA_FILE = Path(r\"./prompt/indicaciones_gestion_requerimiento.json\")\n",
    "ANALYSIS_DIR = Path(r\"./output/greeting_analysis\")\n",
    "OUTPUT_DIR = Path(r\"./data/pomptsft\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_FILE = OUTPUT_DIR / \"dataset_audit.jsonl\"\n",
    "\n",
    "def generate_all_rules_data(n_samples=2500):\n",
    "    \"\"\"Generador masivo que cubre R1 hasta R9 con todas sus variantes.\"\"\"\n",
    "    \n",
    "    names = [\"Juan P√©rez\", \"Mar√≠a Garc√≠a\", \"Carlos Ruiz\", \"Ana Torres\", \"Luis Vega\", \"Elena Sol\"]\n",
    "    dnis = [\"11223344\", \"55667788\", \"99001122\", \"33445566\", \"77889900\"]\n",
    "    dates = [\"01/01/1980\", \"15/05/1992\", \"20/12/1975\", \"10/10/1988\"]\n",
    "    places = [\"Lima\", \"Cusco\", \"Arequipa\", \"Trujillo\", \"Piura\"]\n",
    "    amounts = [\"45.50\", \"89.90\", \"120.00\", \"35.00\", \"150.25\"]\n",
    "    \n",
    "    dataset = []\n",
    "    print(f\"üöÄ Generando {n_samples} ejemplos de entrenamiento con cobertura R1-R9...\")\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        dialog = []\n",
    "        analysis = {}\n",
    "        intent = random.choice([\"BAJA\", \"CONSULTA\", \"RECLAMO\", \"BAJA_PREVIA\"])\n",
    "        name, dni, date, place, amt = [random.choice(lst) for lst in [names, dnis, dates, places, amounts]]\n",
    "        \n",
    "        # --- R1 / R1A: Validaci√≥n ---\n",
    "        if intent == \"BAJA\":\n",
    "            is_compliant = random.choice([True, False])\n",
    "            is_interrupted = random.random() < 0.1 # 10% de probabilidad de corte\n",
    "            \n",
    "            if is_interrupted:\n",
    "                dialog.append(f\"Cliente: Quiero la baja.\\nAsesor: Por favor valide: nombre, dni, fecha, lugar y monto.\\nCliente: {name}, {dni}... espere...\\n(Llamada cortada por cliente)\")\n",
    "                analysis[\"R1_validacion_datos\"] = {\"cumple\": True, \"razon\": \"Asesor aplic√≥ protocolo de baja antes del corte.\", \"score\": 10}\n",
    "                analysis[\"R9_falta_informacion\"] = {\"cumple\": \"NO APLICA\", \"razon\": \"Corte por cliente impide terminar.\", \"score\": 0}\n",
    "            elif is_compliant:\n",
    "                dialog.append(f\"Cliente: Quiero la baja.\\nAsesor: Necesito su Nombre, DNI, Fecha, Lugar y Monto.\\nCliente: {name}, {dni}, {date}, {place}, {amt} soles.\\nAsesor: Validado correctamente.\")\n",
    "                analysis[\"R1_validacion_datos\"] = {\"cumple\": True, \"razon\": \"Validaci√≥n completa realizada.\", \"score\": 10}\n",
    "            else:\n",
    "                dialog.append(f\"Cliente: Quiero dar de baja mi l√≠nea.\\nAsesor: ¬øDNI y nombre?\\nCliente: {dni}, {name}.\\nAsesor: Listo, procederemos.\")\n",
    "                analysis[\"R1_validacion_datos\"] = {\"cumple\": False, \"razon\": \"Omiti√≥ campos obligatorios (fecha, lugar, monto) para baja.\", \"score\": 0}\n",
    "        else:\n",
    "            dialog.append(f\"Cliente: Tengo una duda.\\nAsesor: Nombre y DNI.\\nCliente: {name}, {dni}.\\nAsesor: ¬øEn qu√© le ayudo?\")\n",
    "            analysis[\"R1_validacion_datos\"] = {\"cumple\": True, \"razon\": \"Identificaci√≥n b√°sica suficiente para gesti√≥n no sensible.\", \"score\": 10}\n",
    "\n",
    "        # --- R2: Empat√≠a (Randomly add empathy issues) ---\n",
    "        r2_compliant = random.random() > 0.15\n",
    "        if not r2_compliant:\n",
    "            dialog.append(\"Asesor: (Tono molesto) Ap√∫rese que tengo m√°s llamadas.\\nCliente: No me hable as√≠.\")\n",
    "            analysis[\"R2_empatia_claridad\"] = {\"cumple\": False, \"razon\": \"Asesor muestra falta de cortes√≠a y tono confrontativo.\", \"score\": 0}\n",
    "        else:\n",
    "            analysis[\"R2_empatia_claridad\"] = {\"cumple\": True, \"razon\": \"Mantiene comunicaci√≥n clara y respetuosa.\", \"score\": 10}\n",
    "\n",
    "        # --- R3 / R4 / R8: Retenciones y Rebatimientos ---\n",
    "        if intent == \"BAJA\":\n",
    "            n_offers = random.randint(1, 5)\n",
    "            for i in range(min(n_offers, 3)):\n",
    "                dialog.append(f\"Asesor: ¬øLe interesa la oferta {i+1}?\\nCliente: No gracias.\")\n",
    "            if n_offers > 3:\n",
    "                dialog.append(f\"Asesor: Insisto con la oferta {n_offers}.\\nCliente: Ya dije que no.\")\n",
    "                analysis[\"R3_ofertas_adecuadas\"] = {\"cumple\": False, \"razon\": \"Super√≥ el l√≠mite de 3 ofertas.\", \"score\": 0}\n",
    "                analysis[\"R4_respeto_decision\"] = {\"cumple\": False, \"razon\": \"No respet√≥ la decisi√≥n tras 3 intentos.\", \"score\": 0}\n",
    "                analysis[\"R8_regla_rebatimientos\"] = {\"cumple\": False, \"razon\": \"Super√≥ l√≠mite de rebatimientos permitidos.\", \"score\": 0}\n",
    "            else:\n",
    "                analysis[\"R3_ofertas_adecuadas\"] = {\"cumple\": True, \"razon\": \"Dentro del l√≠mite de 3 ofertas.\", \"score\": 10}\n",
    "                analysis[\"R4_respeto_decision\"] = {\"cumple\": True, \"razon\": \"Respeta decisi√≥n tras ofertas permitidas.\", \"score\": 10}\n",
    "                analysis[\"R8_regla_rebatimientos\"] = {\"cumple\": True, \"razon\": \"Rebatimientos realizados seg√∫n pol√≠tica.\", \"score\": 10}\n",
    "        else:\n",
    "            # Si no es baja, no debe ofrecer retenciones\n",
    "            should_offer = random.choice([True, False]) if intent == \"CONSULTA\" else False\n",
    "            if should_offer:\n",
    "                 dialog.append(\"Asesor: Le ofrezco un descuento.\\nCliente: Pero solo pregunt√© mi saldo.\")\n",
    "                 # En consulta/facilidades no penaliza si ofrece? La regla dice: \n",
    "                 # \"Si no manifiesta deseo de baja... el asesor NO debe ofrecer retenciones... se considera CUMPLE al mantener el contexto\"\n",
    "                 # En el prompt original de la regla dice que \"se considera CUMPLE al mantener la conversaci√≥n dentro del contexto correcto\".\n",
    "                 # Interpretaci√≥n: Si ofrece cuando NO es baja, est√° fuera de contexto? \n",
    "                 # Corrijo: En consulta no debe ofrecer. Si ofrece, es NO CUMPLE en R3.\n",
    "                 analysis[\"R3_ofertas_adecuadas\"] = {\"cumple\": False, \"razon\": \"Ofreci√≥ retenciones en una gesti√≥n que no era de baja.\", \"score\": 0}\n",
    "            else:\n",
    "                 analysis[\"R3_ofertas_adecuadas\"] = {\"cumple\": True, \"razon\": \"No realiz√≥ ofertas fuera de contexto.\", \"score\": 10}\n",
    "\n",
    "        # --- R6A: Baja Previa ---\n",
    "        if intent == \"BAJA_PREVIA\":\n",
    "            r6a_compliant = random.choice([True, False])\n",
    "            if r6a_compliant:\n",
    "                dialog.append(f\"Asesor: El c√≥digo de su baja anterior es {random.randint(111,999)}.\")\n",
    "                analysis[\"R6A_consulta_baja_previa\"] = {\"cumple\": True, \"razon\": \"Brinda informaci√≥n de baja previa correctamente.\", \"score\": 10}\n",
    "            else:\n",
    "                dialog.append(\"Asesor: No veo nada de bajas anteriores en mi pantalla.\")\n",
    "                analysis[\"R6A_consulta_baja_previa\"] = {\"cumple\": False, \"razon\": \"No asisti√≥ con informaci√≥n de gesti√≥n anterior.\", \"score\": 0}\n",
    "\n",
    "        # --- R7: Tiempo de Espera ---\n",
    "        wait_time = random.randint(1, 15)\n",
    "        if wait_time > 5 and intent == \"BAJA\":\n",
    "            dialog.append(f\"(Espera silenciosa de {wait_time} minutos sin motivo)\")\n",
    "            analysis[\"R7_tiempo_espera_justificado\"] = {\"cumple\": False, \"razon\": f\"Espera excesiva de {wait_time} min para dilatar baja.\", \"score\": 0}\n",
    "        else:\n",
    "            analysis[\"R7_tiempo_espera_justificado\"] = {\"cumple\": True, \"razon\": \"Tiempo en espera razonable.\", \"score\": 10}\n",
    "\n",
    "        # --- R5 / R6: Cierre y Resoluci√≥n ---\n",
    "        cuts_at_end = random.random() < 0.05\n",
    "        if cuts_at_end:\n",
    "            dialog.append(\"(Cliente se desconecta antes de formalizar)\")\n",
    "            analysis[\"R5_formalizacion_cierre\"] = {\"cumple\": \"NO APLICA\", \"razon\": \"Corte imprevisto por cliente.\", \"score\": 0}\n",
    "            analysis[\"R6_resolver_consulta_asociada\"] = {\"cumple\": \"NO APLICA\", \"razon\": \"Corte impide resoluci√≥n final.\", \"score\": 0}\n",
    "        else:\n",
    "            r5_compliant = random.choice([True, False])\n",
    "            if r5_compliant:\n",
    "                dialog.append(f\"Asesor: Gesti√≥n realizada. C√≥digo: ID-{random.randint(1,100)}, plazo 24h.\")\n",
    "                analysis[\"R5_formalizacion_cierre\"] = {\"cumple\": True, \"razon\": \"Formaliza con c√≥digo y plazos.\", \"score\": 10}\n",
    "                analysis[\"R6_resolver_consulta_asociada\"] = {\"cumple\": True, \"razon\": \"Resuelve la solicitud principal.\", \"score\": 10}\n",
    "            else:\n",
    "                dialog.append(\"Asesor: Listo, chau.\")\n",
    "                analysis[\"R5_formalizacion_cierre\"] = {\"cumple\": False, \"razon\": \"No entreg√≥ c√≥digo ni explic√≥ plazos.\", \"score\": 0}\n",
    "                analysis[\"R6_resolver_consulta_asociada\"] = {\"cumple\": False, \"razon\": \"No brind√≥ informaci√≥n final necesaria.\", \"score\": 0}\n",
    "\n",
    "        dataset.append({\n",
    "            \"instruction\": \"Genera un reporte de auditor√≠a completo basado en esta transcripci√≥n, evaluando las reglas R1 a R9.\",\n",
    "            \"input\": \"\\n\".join(dialog),\n",
    "            \"output\": json.dumps({\"rule_analysis\": analysis}, ensure_ascii=False)\n",
    "        })\n",
    "    \n",
    "    # Integraci√≥n de data real\n",
    "    if ANALYSIS_DIR.exists():\n",
    "        real_files = list(ANALYSIS_DIR.glob(\"*.json\"))\n",
    "        for f_path in real_files:\n",
    "            try:\n",
    "                with open(f_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if \"transcription_text\" in data and \"rule_analysis\" in data:\n",
    "                    dataset.append({\n",
    "                        \"instruction\": \"Genera un reporte de auditor√≠a completo basado en esta transcripci√≥n.\",\n",
    "                        \"input\": data[\"transcription_text\"],\n",
    "                        \"output\": json.dumps({\"rule_analysis\": data[\"rule_analysis\"]}, ensure_ascii=False)\n",
    "                    })\n",
    "            except: continue\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    with open(DATASET_FILE, 'w', encoding='utf-8') as f:\n",
    "        for entry in dataset:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"üéâ Dataset total construido con {len(dataset)} ejemplos (sint√©ticos + reales).\")\n",
    "    print(f\"üìÅ Archivo guardado en: {DATASET_FILE}\")\n",
    "\n",
    "generate_all_rules_data(n_samples=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üõ†Ô∏è Configuraci√≥n del Entorno\n",
    "Instalaci√≥n de dependencias optimizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\n",
    "!pip install --no-deps \"xformers<0.0.29\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install pandas datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üß† Carga del Modelo Base\n",
    "Cargamos el modelo en 4 bits para ahorrar memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unsloth: torch==2.9.1 requires torchvision>=0.24.0, but found torchvision==0.17.0+cu118. Please refer to https://pytorch.org/get-started/previous-versions/ for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m \u001b[38;5;66;03m# Sube a 4096 si tienes mucha VRAM\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\unsloth\\__init__.py:38\u001b[39m\n\u001b[32m     36\u001b[39m fix_message_factory_issue()\n\u001b[32m     37\u001b[39m check_fbgemm_gpu_version()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mtorchvision_compatibility_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m fix_diffusers_warnings()\n\u001b[32m     40\u001b[39m fix_huggingface_hub()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\unsloth\\import_fixes.py:450\u001b[39m, in \u001b[36mtorchvision_compatibility_check\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Version(torchvision_version) < Version(required_torchvision):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: torch==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires torchvision>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_torchvision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found torchvision==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorchvision_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease refer to https://pytorch.org/get-started/previous-versions/ for more information.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    454\u001b[39m     )\n\u001b[32m    456\u001b[39m logger.info(\n\u001b[32m    457\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: torch==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and torchvision==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorchvision_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are compatible.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: Unsloth: torch==2.9.1 requires torchvision>=0.24.0, but found torchvision==0.17.0+cu118. Please refer to https://pytorch.org/get-started/previous-versions/ for more information."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Sube a 4096 si tienes mucha VRAM\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\" # O \"unsloth/Qwen2.5-7B-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Agregar adaptadores LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üìä Preparaci√≥n de Datos\n",
    "Convertimos los JSON del pipeline anterior al formato **Alpaca**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando dataset desde: data\\pomptsft\\dataset_audit.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74e3edb31de4d98b042112cc3ce535f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Ejecutar la carga\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m dataset = \u001b[43mload_and_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Mostrar un ejemplo del texto final que ver√° el LLM\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mload_and_prepare_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m, data_files=\u001b[38;5;28mstr\u001b[39m(DATASET_FILE), split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Aplicar el formateo de Alpaca (mapeo para el entrenamiento)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatting_prompts_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset cargado y formateado con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ejemplos.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3332\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3331\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3332\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3335\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3688\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3687\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3688\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3690\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3638\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3636\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3637\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3638\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proy\\AntiG\\MIA\\audio_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3561\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3559\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3560\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3561\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3562\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mformatting_prompts_func\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     27\u001b[39m texts = []\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m instruction, input_text, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(instructions, inputs, outputs):\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Formatear cada ejemplo con el template de Alpaca\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     text = ALPACA_TEMPLATE.format(instruction, input_text, output) + \u001b[43mtokenizer\u001b[49m.eos_token\n\u001b[32m     31\u001b[39m     texts.append(text)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m : texts, }\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# üîß Configuraci√≥n de rutas\n",
    "DATA_DIR = Path(\"./data/pomptsft\")\n",
    "DATASET_FILE = DATA_DIR / \"dataset_audit.jsonl\"\n",
    "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{} \n",
    "\n",
    "### Input:\n",
    "{} \n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Formatear cada ejemplo con el template de Alpaca\n",
    "        text = ALPACA_TEMPLATE.format(instruction, input_text, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "def load_and_prepare_dataset():\n",
    "    print(f\"üìÇ Cargando dataset desde: {DATASET_FILE}\")\n",
    "    \n",
    "    if not DATASET_FILE.exists():\n",
    "        print(\"‚ùå ERROR: El archivo de dataset no existe. Ejecuta el Punto 0 primero.\")\n",
    "        return None\n",
    "        \n",
    "    # Cargar usando la librer√≠a datasets de HuggingFace\n",
    "    dataset = load_dataset(\"json\", data_files=str(DATASET_FILE), split=\"train\")\n",
    "    \n",
    "    # Aplicar el formateo de Alpaca (mapeo para el entrenamiento)\n",
    "    dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset cargado y formateado con {len(dataset)} ejemplos.\")\n",
    "    return dataset\n",
    "\n",
    "# Ejecutar la carga\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Mostrar un ejemplo del texto final que ver√° el LLM\n",
    "if dataset:\n",
    "    print(\"\\n--- MUESTRA DEL FORMATO FINAL ---\")\n",
    "    print(dataset[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üî• Entrenamiento\n",
    "Ejecuci√≥n del ciclo de fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=OUTPUT_DATASET, split=\"train\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", # O usar formato Alpaca directo\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # Ajustar seg√∫n tama√±o de dataset\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üì¶ Exportaci√≥n a GGUF (LM Studio)\n",
    "Guardar el modelo para usarlo localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en formato GGUF (esto permite cargarlo en LM Studio)\n",
    "model.save_pretrained_gguf(\"model_audit_q4\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "print(\"‚úÖ Modelo exportado correctamente a 'model_audit_q4'\")\n",
    "print(\"Busca el archivo .gguf para cargarlo en LM Studio.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
